{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual keywords generation for RAG\n",
        "\n",
        "**Problem**: Independent chunking in traditional RAG systems leads to the loss of contextual information between chunks. This makes it difficult for LLMs to retrieve relevant data when context (e.g., the subject or entity being discussed) is not explicitly repeated within individual chunks.\n",
        "\n",
        "**Solution**: Generate keywords for each chunk to fulfill missing contextual information. These keywords (e.g., \"BMW, X5, pricing\") enrich the chunk with necessary context, ensuring better retrieval accuracy. By embedding this enriched metadata, the system bridges gaps between related chunks, enabling effective query matching and accurate answer generation.\n",
        "\n",
        "[This article](https://medium.com/@ailabs/overcoming-independent-chunking-in-rag-systems-a-hybrid-approach-5d2c205b3732) explains benefits of contextual chunking.\n",
        "\n",
        "**Note** This method does not require calling LLM for each chunk separately, which makes it efficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "nUWgVdTv67ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Let's start by installing the important packages.\n",
        "For Llama model inference, we use DeepInfra here, but you can use any inference service provider"
      ],
      "metadata": {
        "id": "Wt3pZ_J2tKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "DEEPINFRA_API_KEY = \"\" #your api key"
      ],
      "metadata": {
        "id": "LEz4Dfa9-i-Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, get your document content. Preferred document size is between 2k and 20k"
      ],
      "metadata": {
        "id": "mwW6s7EJm9ul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Iooj2f76wXl"
      },
      "outputs": [],
      "source": [
        "document_content = \"\"\n",
        "with open('llama_article.txt', 'r') as file:\n",
        "    document_content = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then split the document content into chunks of 300-1000 tokens"
      ],
      "metadata": {
        "id": "q4IO9wIip8lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split into chunks (simple way)\n",
        "def split_into_chunks(content, chunk_size):\n",
        "\timport tiktoken\n",
        "\tenc = tiktoken.get_encoding(\"o200k_base\")\n",
        "\ta = enc.encode(content)\n",
        "\tleft, chunks = 0, []\n",
        "\twhile left < len(a):\n",
        "\t\tarr = a[left : left+chunk_size]\n",
        "\t\tchunks.append(enc.decode(arr))\n",
        "\t\tleft+=chunk_size\n",
        "\treturn chunks\n",
        "\n",
        "chunks = split_into_chunks(document_content, 400)\n",
        "\n",
        "#generate chunked content\n",
        "chunked_content = \"\"\n",
        "for idx, text in enumerate(chunks):\n",
        "  chunked_content+=f\"### Chunk {idx+1} ###\\n{text}\\n\\n\"\n"
      ],
      "metadata": {
        "id": "oek9g5Xom2XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now your chunked_content looks like\n",
        "\n",
        "```\n",
        "### Chunk 1 ###\n",
        "{chunk1}\n",
        "\n",
        "### Chunk 2 ###\n",
        "{chunk2}\n",
        "\n",
        "..\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "J4evTJGL83P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, generate contextual keywords to have better chunk representation for embeddings. Here, we use DeepInfra servers for inference"
      ],
      "metadata": {
        "id": "iTBzN--tmIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "openai = OpenAI(api_key=DEEPINFRA_API_KEY, base_url=\"https://api.deepinfra.com/v1/openai\")\n",
        "\n",
        "def deepinfra_run(system_prompt, user_message):\n",
        "\tchat_completion = openai.chat.completions.create(\n",
        "\t    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
        "\t    messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_message}],\n",
        "\t    max_tokens=4096\n",
        "\t)\n",
        "\treturn chat_completion.choices[0].message.content\n",
        "\n",
        "system_prompt = '''\n",
        "Each chunk is separated as ### Chunk [id] ###. For each chunk generate keywords required to fully understand the chunk without any need for looking at the previous chunks.\n",
        "Don't just say \"List of services\", because its unclear what services are you referring to. Make sure to cover all chunks.\n",
        "Sample output:\n",
        "Chunk 1: BMW X5, pricings in France\n",
        "Chunk 2: BMW X5, discounts\n",
        "'''\n",
        "\n",
        "keywords_st = deepinfra_run(system_prompt, chunked_content)\n",
        "print(keywords_st)"
      ],
      "metadata": {
        "id": "2AqW8zgw9Ah2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to parse the generated keywords into array\n"
      ],
      "metadata": {
        "id": "ucmJJjvuqpSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def parse_keywords(content):\n",
        "    result = []\n",
        "    lines = content.strip().split('\\n')\n",
        "    current_chunk = []\n",
        "    inline_pattern = re.compile(r'^\\s*[^#:]+\\s*:\\s*(.+)$')  # Matches lines like \"Chunk1: word1, word2\"\n",
        "    section_pattern = re.compile(r'^###\\s*[^#]+\\s*###$')    # Matches lines like \"### Chunk1 ###\"\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        inline_match = inline_pattern.match(line)\n",
        "\n",
        "        if inline_match:\n",
        "            words_str = inline_match.group(1)\n",
        "            words = [word.strip() for word in words_str.split(',') if word.strip()]\n",
        "            result.append(words)\n",
        "            continue\n",
        "\n",
        "        if section_pattern.match(line):\n",
        "            if current_chunk:\n",
        "                result.append(current_chunk)\n",
        "                current_chunk = []\n",
        "            continue\n",
        "\n",
        "        if current_chunk is not None:\n",
        "            words = [word.strip() for word in line.split(',') if word.strip()]\n",
        "            current_chunk.extend(words)\n",
        "\n",
        "    if current_chunk:\n",
        "      result.append(current_chunk)\n",
        "    return result\n",
        "\n",
        "\n",
        "keywords = parse_keywords(keywords_st)\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "ikTy7nA8AFGz",
        "outputId": "e8fba18d-a697-472e-b5cd-cb71f23c8a0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keywords_st' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-676c58e13af2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords_st\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keywords_st' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can modify the chunks using the generated keywords.\n",
        "\n",
        "```\n",
        "For example,\n",
        "chunk1 = #{keywords1}\\n{chunk1}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "X1myu--0kUi0"
      }
    }
  ]
}