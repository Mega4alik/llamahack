{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9aca84-89dc-4a58-9079-d9fb3ef78197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from config import OPENAI_API_KEY, LLAMAPARSE_API_KEY\n",
    "\n",
    "# Enable nested async loops\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adeed54-5415-4b7e-83f8-ba92fe9c494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 56b65a50-1400-4e58-a76d-6780185a01fd\n",
      "pdf file pages: 160\n"
     ]
    }
   ],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key=LLAMAPARSE_API_KEY,\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_files=['./data/3M_2018_10K.pdf'], file_extractor=file_extractor).load_data()\n",
    "print(\"pdf file pages:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562350c-ef4a-4657-856e-16e2c5da93cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating questions..\n"
     ]
    }
   ],
   "source": [
    "#split into chunks (by tokens)\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from helper import file_get_contents, file_put_contents, generate_contextual_keywords, get_llm_answer, generate_questions_bychunk\n",
    "from llama_index.core.schema import Document\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "def split_into_chunks(content, chunk_size):\n",
    "\ta = enc.encode(content)\n",
    "\tleft, chunks = 0, []\n",
    "\twhile left < len(a):\n",
    "\t\tarr = a[left : left+chunk_size]\n",
    "\t\tchunks.append(enc.decode(arr))\n",
    "\t\tleft+=chunk_size\n",
    "\treturn chunks\n",
    "    \n",
    "def generate_chunked_content(chunks):\n",
    "    chunked_content = \"\"\n",
    "    for idx, text in enumerate(chunks):\n",
    "      chunked_content+=f\"### Chunk {idx+1} ###\\n{text}\\n\\n\"\n",
    "    return chunked_content\n",
    "    \n",
    "\n",
    "#generate contextual keywords\n",
    "path = \"./temp/chunks2.json\"\n",
    "if not os.path.exists(path):\n",
    "    print(\"Generating keywords..\")\n",
    "    document_content, chunks, chunks2 = \"\", [], []\n",
    "    for doc in documents: document_content+=doc.text+\"\\n\"\n",
    "    chunks1 = split_into_chunks(document_content, 400)    \n",
    "    for i, chunk in enumerate(chunks1):        \n",
    "        chunks.append(chunk)\n",
    "        if (len(chunks) > 10 or (i==len(chunks1)-1) and len(chunks)>2):\n",
    "            chunked_content = generate_chunked_content(chunks)\n",
    "            keywords = generate_contextual_keywords(chunked_content)        \n",
    "            print(\"page_end:\", i+1, keywords, len(keywords), len(chunks))            \n",
    "            assert len(keywords) >= len(chunks)\n",
    "            for j in range(len(chunks)): chunks2.append( {\"keywords\":keywords[j], \"content\":chunks[j]} )\n",
    "            chunks = []\n",
    "    file_put_contents(path, json.dumps(chunks2))\n",
    "else:\n",
    "    chunks2 = json.loads(file_get_contents(path)) #it has content, keywords\n",
    "\n",
    "\n",
    "#generate questions\n",
    "path = \"./temp/chunks3.json\"\n",
    "if not os.path.exists(path):\n",
    "    print(\"Generating questions..\")\n",
    "    chunks3 = generate_questions_bychunk(chunks2) \n",
    "    file_put_contents(path, json.dumps(chunks3))\n",
    "else:\n",
    "    chunks3 = json.loads(file_get_contents(path)) #it has content, keywords, questions, idx now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40bb95-90d5-49d8-8374-8ab0f4c3fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Index\n",
    "from llama_index.core import GPTVectorStoreIndex, StorageContext, load_index_from_storage\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "INDEX_DIR = \"./temp/local_index_cache\"\n",
    "if not os.path.exists(INDEX_DIR):\n",
    "    print(\"Creating new index ...\")\n",
    "    documents2 = [Document(text=\"#\"+\", \".join(x[\"keywords\"])+\"\\n\"+x[\"content\"], id_=str(x[\"idx\"]), metadata={\"id\": str(x[\"idx\"])}) for x in chunks3]\n",
    "    index = GPTVectorStoreIndex.from_documents(documents2)\n",
    "    index.storage_context.persist(persist_dir=INDEX_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# run tests\n",
    "count, correct = 0, 0\n",
    "for test in chunks3[:]:\n",
    "    idx = test[\"idx\"]\n",
    "    for question in test[\"questions\"]:\n",
    "        count+=1\n",
    "        response = query_engine.query(question)\n",
    "        print(\"\\n\\n--- Test:\", question, \"idx:\", idx)\n",
    "        for result in response.source_nodes[:3]: \n",
    "            print(result.node.metadata) #prompt+=f\"\\n\\n<Document>\\n                     \n",
    "            if result.node.metadata['id'] == str(idx): correct+=1                \n",
    "\n",
    "print(\"Test correct, all:\", correct, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
