{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9aca84-89dc-4a58-9079-d9fb3ef78197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from config import OPENAI_API_KEY, LLAMAPARSE_API_KEY\n",
    "\n",
    "# Enable nested async loops\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adeed54-5415-4b7e-83f8-ba92fe9c494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id ebcb069c-85ab-432f-80d5-79b287b0d949\n",
      "..........."
     ]
    }
   ],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key=LLAMAPARSE_API_KEY,\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_files=['./data/3M_2018_10K.pdf'], file_extractor=file_extractor).load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562350c-ef4a-4657-856e-16e2c5da93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into chunks (by tokens)\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from helper import file_get_contents, file_put_contents, generate_contextual_keywords, get_llm_answer, generate_questions_bychunk\n",
    "from llama_index.core.schema import Document\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "def split_into_chunks(content, chunk_size):\n",
    "\ta = enc.encode(content)\n",
    "\tleft, chunks = 0, []\n",
    "\twhile left < len(a):\n",
    "\t\tarr = a[left : left+chunk_size]\n",
    "\t\tchunks.append(enc.decode(arr))\n",
    "\t\tleft+=chunk_size\n",
    "\treturn chunks\n",
    "    \n",
    "def generate_chunked_content(chunks):\n",
    "    chunked_content = \"\"\n",
    "    for idx, text in enumerate(chunks):\n",
    "      chunked_content+=f\"### Chunk {idx+1} ###\\n{text}\\n\\n\"\n",
    "    return chunked_content\n",
    "    \n",
    "\n",
    "#generate contextual keywords\n",
    "if not os.path.exists(\"./temp/chunks2.json\"):\n",
    "    document_content, chunks2 = \"\", []\n",
    "    for i, doc in enumerate(documents):\n",
    "        document_content+=doc.text+\"\\n\"\n",
    "        if (len(document_content) > 15000 or i==len(documents)-1):\n",
    "            chunks = split_into_chunks(document_content, 400)        \n",
    "            chunked_content = generate_chunked_content(chunks)\n",
    "            keywords = generate_contextual_keywords(chunked_content)        \n",
    "            document_content = \"\"\n",
    "            print(\"page_end:\", i+1, keywords, len(keywords), len(chunks))\n",
    "            assert len(keywords) == len(chunks)\n",
    "            for j in range(len(chunks)):\n",
    "                chunks2.append( {\"keywords\":keywords[j], \"content\":chunks[j]} )\n",
    "            break    \n",
    "    file_put_contents(\"./temp/chunks2.json\", json.dumps(chunks2))\n",
    "else:\n",
    "    chunks2 = json.loads(file_get_contents(\"./temp/chunks2.json\")) #it has content, keywords\n",
    "\n",
    "\n",
    "#generate questions\n",
    "path = \"./temp/chunks3.json\"\n",
    "if not os.path.exists(path):\n",
    "    chunks3 = generate_questions_bychunk(chunks2) \n",
    "    file_put_contents(path, json.dumps(chunks3))\n",
    "else:\n",
    "    chunks3 = json.loads(file_get_contents(path)) #it has content, keywords, questions, idx now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40bb95-90d5-49d8-8374-8ab0f4c3fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Index\n",
    "from llama_index.core import GPTVectorStoreIndex, StorageContext, load_index_from_storage\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "INDEX_DIR = \"./temp/local_index_cache\"\n",
    "if not os.path.exists(INDEX_DIR):\n",
    "    print(\"Creating new index ...\")\n",
    "    documents2 = [Document(text=\"#\"+x[\"keywords\"]+\"\\n\"+x[\"content\"] for x in chunks2]\n",
    "    index = GPTVectorStoreIndex.from_documents(documents2)\n",
    "    index.storage_context.persist(persist_dir=INDEX_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# run queries from test set\n",
    "lines = file_get_contents('./data/financebench_open_source.jsonl').split('\\n')\n",
    "tests = [ json.loads(line) for line in lines  if line]\n",
    "tests = [q for q in tests if q[\"doc_name\"]==\"3M_2018_10K\"]\n",
    "print(len(tests))\n",
    "for test in tests[:1]:\n",
    "    question, answer, prompt = test[\"question\"], test[\"answer\"], \"\"\n",
    "    response = query_engine.query(question)\n",
    "    print(\"\\n\\n--- Test:\", question, \"\\nA:\", answer)\n",
    "    for result in response.source_nodes[:5]: prompt+=f\"\\n\\n<Document>\\n{result.node.text}\"\n",
    "    gen_answer = get_llm_answer(prompt, question)\n",
    "    print(\"Generated_answer:\", gen_answer)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
